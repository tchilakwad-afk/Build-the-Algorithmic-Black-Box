# -*- coding: utf-8 -*-
"""WiDS_Week3_Day10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xfyJ7CbVtosgAysD0DxmHFzk0FAU9cjW
"""

!pip install stable-baselines3 gymnasium

import numpy as np

import gymnasium as gym
from gymnasium import spaces

from collections import defaultdict
from collections import deque

from stable_baselines3 import PPO

class OrderBookMarket:
    def __init__(self, initial_price=100.0, tick_size=0.1):
        self.last_price = initial_price
        self.tick_size = tick_size
        self.bids = defaultdict(float)
        self.asks = defaultdict(float)

    def flush_book(self):
        self.bids.clear()
        self.asks.clear()

    def get_mid_price(self):
        best_bid = max(self.bids.keys()) if self.bids else self.last_price - 0.5
        best_ask = min(self.asks.keys()) if self.asks else self.last_price + 0.5
        return (best_bid + best_ask) / 2.0

    def post_limit_order(self, side, price, quantity):
        price = round(price / self.tick_size) * self.tick_size
        if side == "buy": self.bids[price] += quantity
        elif side == "sell": self.asks[price] += quantity

    def match_market_order(self, side, quantity):
        remaining = quantity
        exec_price = self.last_price

        if side == "buy":
            sorted_asks = sorted(self.asks.keys())
            for p in sorted_asks:
                if remaining <= 0: break
                matched = min(remaining, self.asks[p])
                self.asks[p] -= matched
                if self.asks[p] < 1e-9: del self.asks[p]
                remaining -= matched
                exec_price = p

        elif side == "sell":
            sorted_bids = sorted(self.bids.keys(), reverse=True)
            for p in sorted_bids:
                if remaining <= 0: break
                matched = min(remaining, self.bids[p])
                self.bids[p] -= matched
                if self.bids[p] < 1e-9: del self.bids[p]
                remaining -= matched
                exec_price = p

        self.last_price = exec_price
        return exec_price

class LOBTradingEnv(gym.Env):
  def __init__(self):
    super(LOBTradingEnv, self).__init__()

    self.action_space = spaces.Discrete(3)

    self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(5,), dtype=np.float32)

    self.market = OrderBookMarket()
    self.initial_cash = 10_000

  def reset(self, seed=None, options=None):
    super().reset(seed=seed)
    self.market = OrderBookMarket()
    self.inventory = 0
    self.cash = self.initial_cash
    self.step_count = 0

    self.momentum = 0.0

    self._generate_background_liquidity()

    return self._get_obs(), {}

  def _generate_background_liquidity(self):
    self.market.flush_book()
    mid = self.market.last_price

    self.momentum = (0.8 * self.momentum) + np.random.normal(0, 0.5)
    mid += self.momentum

    self.market.last_price = mid

    for i in range(1, 6):
      self.market.post_limit_order("sell", mid + i*0.2, 10)
    for i in range(1, 6):
      self.market.post_limit_order("buy", mid - i*0.2, 10)

  def _get_obs(self):
    mid_price = self.market.get_mid_price()
    portfolio_val = self.cash + (self.inventory * mid_price)

    return np.array([
        self.inventory / 10.0,
        self.cash / 10_000.0,
        mid_price / 100.0,
        portfolio_val / 10_000.0,
        self.momentum
    ], dtype=np.float32)

  def step(self, action):
    mid_before = self.market.get_mid_price()
    prev_val = self.cash + (self.inventory * mid_before)

    self._generate_background_liquidity()
    self.step_count += 1

    if action == 1:
      if self.cash > self.market.last_price:
        exec_price = self.market.match_market_order("buy", 1)
        self.inventory += 1
        self.cash -= exec_price
    elif action == 2:
      if self.inventory > -5:
        exec_price = self.market.match_market_order("sell", 1)
        self.inventory -= 1
        self.cash += exec_price

    mid_after = self.market.get_mid_price()
    current_val = self.cash + (self.inventory * mid_after)

    reward = current_val - prev_val

    reward -= 0.001 * abs(self.inventory)

    terminated = self.step_count >= 200
    truncated = False
    return self._get_obs(), reward, terminated, truncated, {}

def compute_returns(equity_curve):
  equity_curve = np.array(equity_curve)
  returns = np.diff(equity_curve) / equity_curve[:-1]
  return returns

def sharpe_ratio(returns, eps = 1e-8):
  if returns.std() < eps:
    return 0.0
  return returns.mean() / returns.std()

def max_drawdown(equity_curve):
  equity = np.array(equity_curve)
  peak = np.maximum.accumulate(equity)
  drawdown = (peak - equity) / peak
  return np.max(drawdown)

def run_episode(env, policy_fn):
  obs, _ = env.reset()
  terminated, truncated = False, False

  equity_curve = [obs[3] * 10_000.0]

  while not (terminated or truncated):
    action = policy_fn(obs)
    obs, reward, terminated, truncated, _ = env.step(action)
    equity_curve.append(obs[3] * 10_000.0)

  return equity_curve

def buy_and_hold_policy():
  first = True
  def policy(obs):
    nonlocal first
    if first:
      first = False
      return 1
    return 0
  return policy

def random_policy():
  def policy(obs):
    return np.random.choice([0, 1, 2])
  return policy

def rl_policy(model):
  def policy(obs):
    action, _ = model.predict(obs, deterministic = True)
    return action
  return policy

def evaluate_agent(env_fn, policy_factory, n_episodes = 20):
  sharpe_vals = []
  drawdowns = []

  for i in range(n_episodes):
    env = env_fn()
    policy = policy_factory()

    equity = run_episode(env, policy)

    returns = np.diff(equity) / equity[:-1]
    sharpe_vals.append(sharpe_ratio(returns))
    drawdowns.append(max_drawdown(equity))

  return {
      "sharpe_mean": np.mean(sharpe_vals),
      "max_dd_mean": np.mean(drawdowns)
  }

train_env = LOBTradingEnv()

best_model = PPO(
    "MlpPolicy",
    train_env,
    learning_rate = 8.168455894760161e-05,
    gamma = 0.9777324201779083,
    ent_coef = 0.00025081156860452336,
    n_steps = 2048,
    batch_size = 64,
    verbose = 0
)

best_model.learn(total_timesteps = 100_000)

env_fn = lambda: LOBTradingEnv()

results = {}

results["RL Agent"] = evaluate_agent(
    env_fn,
    lambda: rl_policy(best_model),
    n_episodes = 20
)

results["Buy & Hold"] = evaluate_agent(
    env_fn,
    buy_and_hold_policy,
    n_episodes = 20
)

results["Random"] = evaluate_agent(
    env_fn,
    random_policy,
    n_episodes = 20
)

for k, v in results.items():
  print(k, v)

import matplotlib.pyplot as plt

def run_policy_trajectory(env_fn, policy_fn):
  env = env_fn()
  obs, _ = env.reset()
  terminated = truncated = False

  equity_curve = [obs[3] * 10_000.0]
  peak = equity_curve[0]
  drawdown_curve = [0.0]

  while not (terminated or truncated):
    action = policy_fn(obs)
    obs, reward, terminated, truncated, _ = env.step(action)

    equity = obs[3] * 10_000.0
    equity_curve.append(equity)

    peak = max(peak, equity)
    drawdown_curve.append((peak - equity) / peak)

  return np.array(equity_curve), np.array(drawdown_curve)

rl_equity, rl_dd = run_policy_trajectory(env_fn, rl_policy(best_model))

bh_equity, bh_dd = run_policy_trajectory(env_fn, buy_and_hold_policy())

rand_equity, rand_dd = run_policy_trajectory(env_fn, random_policy())

plt.figure(figsize = (10, 5))
plt.plot(rl_equity, label = "RL Agent")
plt.plot(bh_equity, label = "Buy & Hold")
plt.plot(rand_equity, label = "Random")

plt.title("Equity Curves")
plt.xlabel("Time Step")
plt.ylabel("Portfolio Value")
plt.legend()
plt.grid(True)
plt.show()

plt.figure(figsize = (10, 5))
plt.plot(rl_dd, label = "RL Agent")
plt.plot(bh_dd, label = "Buy & Hold")
plt.plot(rand_dd, label = "Random")

plt.title("Drawdown Curves")
plt.xlabel("Time Step")
plt.ylabel("Drawdown")
plt.legend()
plt.grid(True)
plt.show()