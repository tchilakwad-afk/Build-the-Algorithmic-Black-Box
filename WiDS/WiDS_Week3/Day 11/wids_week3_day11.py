# -*- coding: utf-8 -*-
"""WiDS_Week3_Day11.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fSOUTLkLJcI5sRy5Sxv3u5PkfgS-lu4K
"""

!pip install stable-baselines3 gymnasium

import numpy as np

import gymnasium as gym
from gymnasium import spaces

from collections import defaultdict
from collections import deque

from stable_baselines3 import PPO

class OrderBookMarket:
    def __init__(self, initial_price=100.0, tick_size=0.1):
        self.last_price = initial_price
        self.tick_size = tick_size
        self.bids = defaultdict(float)
        self.asks = defaultdict(float)

    def flush_book(self):
        self.bids.clear()
        self.asks.clear()

    def get_mid_price(self):
        best_bid = max(self.bids.keys()) if self.bids else self.last_price - 0.5
        best_ask = min(self.asks.keys()) if self.asks else self.last_price + 0.5
        return (best_bid + best_ask) / 2.0

    def post_limit_order(self, side, price, quantity):
        price = round(price / self.tick_size) * self.tick_size
        if side == "buy": self.bids[price] += quantity
        elif side == "sell": self.asks[price] += quantity

    def match_market_order(self, side, quantity):
        remaining = quantity
        exec_price = self.last_price

        if side == "buy":
            sorted_asks = sorted(self.asks.keys())
            for p in sorted_asks:
                if remaining <= 0: break
                matched = min(remaining, self.asks[p])
                self.asks[p] -= matched
                if self.asks[p] < 1e-9: del self.asks[p]
                remaining -= matched
                exec_price = p

        elif side == "sell":
            sorted_bids = sorted(self.bids.keys(), reverse=True)
            for p in sorted_bids:
                if remaining <= 0: break
                matched = min(remaining, self.bids[p])
                self.bids[p] -= matched
                if self.bids[p] < 1e-9: del self.bids[p]
                remaining -= matched
                exec_price = p

        self.last_price = exec_price
        return exec_price

class LOBTradingEnv(gym.Env):
  def __init__(self):
    super(LOBTradingEnv, self).__init__()

    self.action_space = spaces.Discrete(3)

    self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(5,), dtype=np.float32)

    self.market = OrderBookMarket()
    self.initial_cash = 10_000

  def reset(self, seed=None, options=None):
    super().reset(seed=seed)
    self.market = OrderBookMarket()
    self.inventory = 0
    self.cash = self.initial_cash
    self.step_count = 0

    self.momentum = 0.0

    self._generate_background_liquidity()

    return self._get_obs(), {}

  def _generate_background_liquidity(self):
    self.market.flush_book()
    mid = self.market.last_price

    self.momentum = (0.8 * self.momentum) + np.random.normal(0, 0.5)
    mid += self.momentum

    self.market.last_price = mid

    for i in range(1, 6):
      self.market.post_limit_order("sell", mid + i*0.2, 10)
    for i in range(1, 6):
      self.market.post_limit_order("buy", mid - i*0.2, 10)

  def _get_obs(self):
    mid_price = self.market.get_mid_price()
    portfolio_val = self.cash + (self.inventory * mid_price)

    return np.array([
        self.inventory / 10.0,
        self.cash / 10_000.0,
        mid_price / 100.0,
        portfolio_val / 10_000.0,
        self.momentum
    ], dtype=np.float32)

  def step(self, action):
    mid_before = self.market.get_mid_price()
    prev_val = self.cash + (self.inventory * mid_before)

    self._generate_background_liquidity()
    self.step_count += 1

    max_inventory = 5

    if action == 1:
      if self.cash > self.market.last_price and self.inventory < max_inventory:
        exec_price = self.market.match_market_order("buy", 1)
        self.inventory += 1
        self.cash -= exec_price
    elif action == 2:
      if self.inventory > -max_inventory:
        exec_price = self.market.match_market_order("sell", 1)
        self.inventory -= 1
        self.cash += exec_price

    mid_after = self.market.get_mid_price()
    current_val = self.cash + (self.inventory * mid_after)

    reward = current_val - prev_val

    reward -= 0.001 * abs(self.inventory)

    terminated = self.step_count >= 200
    truncated = False
    return self._get_obs(), reward, terminated, truncated, {}

def compute_returns(equity_curve):
  equity_curve = np.array(equity_curve)
  returns = np.diff(equity_curve) / equity_curve[:-1]
  return returns

def sharpe_ratio(returns, eps = 1e-8):
  if returns.std() < eps:
    return 0.0
  return returns.mean() / returns.std()

def max_drawdown(equity_curve):
  equity = np.array(equity_curve)
  peak = np.maximum.accumulate(equity)
  drawdown = (peak - equity) / peak
  return np.max(drawdown)

def run_episode(env, policy_fn):
  obs, _ = env.reset()
  terminated, truncated = False, False

  equity_curve = [obs[3] * 10_000.0]

  while not (terminated or truncated):
    action = policy_fn(obs)
    obs, reward, terminated, truncated, _ = env.step(action)
    equity_curve.append(obs[3] * 10_000.0)

  return equity_curve

def buy_and_hold_policy():
  first = True
  def policy(obs):
    nonlocal first
    if first:
      first = False
      return 1
    return 0
  return policy

def random_policy():
  def policy(obs):
    return np.random.choice([0, 1, 2])
  return policy

def rl_policy(model):
  def policy(obs):
    action, _ = model.predict(obs, deterministic = True)
    return action
  return policy

def evaluate_agent(env_fn, policy_factory, n_episodes = 20):
  sharpe_vals = []
  drawdowns = []

  for i in range(n_episodes):
    env = env_fn()
    policy = policy_factory()

    equity = run_episode(env, policy)

    returns = np.diff(equity) / equity[:-1]
    sharpe_vals.append(sharpe_ratio(returns))
    drawdowns.append(max_drawdown(equity))

  return {
      "sharpe_mean": np.mean(sharpe_vals),
      "max_dd_mean": np.mean(drawdowns)
  }

train_env = LOBTradingEnv()

best_model = PPO(
    "MlpPolicy",
    train_env,
    learning_rate = 8.168455894760161e-05,
    gamma = 0.9777324201779083,
    ent_coef = 0.00025081156860452336,
    n_steps = 2048,
    batch_size = 64,
    verbose = 0
)

best_model.learn(total_timesteps = 100_000)

env_fn = lambda: LOBTradingEnv()

results = {}

results["RL Agent"] = evaluate_agent(
    env_fn,
    lambda: rl_policy(best_model),
    n_episodes = 20
)

results["Buy & Hold"] = evaluate_agent(
    env_fn,
    buy_and_hold_policy,
    n_episodes = 20
)

results["Random"] = evaluate_agent(
    env_fn,
    random_policy,
    n_episodes = 20
)

for k, v in results.items():
  print(k, v)

import matplotlib.pyplot as plt

def run_policy_trajectory(env_fn, policy_fn):
  env = env_fn()
  obs, _ = env.reset()
  terminated = truncated = False

  equity_curve = []
  drawdown_curve = []

  price_series = []
  actions = []
  pnl_series = []

  peak = None
  prev_equity = None

  while not (terminated or truncated):
    mid_price = env.market.get_mid_price()
    price_series.append(mid_price)

    action = policy_fn(obs)
    actions.append(action)

    obs, reward, terminated, truncated, _ = env.step(action)

    equity = obs[3] * 10_000.0
    equity_curve.append(equity)

    if prev_equity is None:
      pnl = 0.0
      peak = equity
    else:
      pnl = equity - prev_equity

    pnl_series.append(pnl)
    prev_equity = equity

    peak = max(peak, equity)
    drawdown_curve.append((peak - equity) / peak)

  return(
      np.array(equity_curve),
      np.array(drawdown_curve),
      np.array(price_series),
      np.array(actions),
      np.array(pnl_series)
  )

rl_equity, rl_dd, price_series, rl_actions, rl_pnl = run_policy_trajectory(env_fn, rl_policy(best_model))

bh_equity, bh_dd, _, _, _ = run_policy_trajectory(env_fn, buy_and_hold_policy())

rand_equity, rand_dd, _, _, _ = run_policy_trajectory(env_fn, random_policy())

rl_returns = np.diff(rl_equity) / rl_equity[:-1]
rl_sharpe = sharpe_ratio(rl_returns)
rl_max_dd = np.max(rl_dd)

plt.figure(figsize = (10, 5))
plt.plot(rl_equity, label = "RL Agent")
plt.plot(bh_equity, label = "Buy & Hold")
plt.plot(rand_equity, label = "Random")

plt.title("Equity Curves")
plt.xlabel("Time Step")
plt.ylabel("Portfolio Value")
plt.legend()
plt.grid(True)
plt.show()

plt.figure(figsize = (10, 5))
plt.plot(rl_dd, label = "RL Agent")
plt.plot(bh_dd, label = "Buy & Hold")
plt.plot(rand_dd, label = "Random")

plt.title("Drawdown Curves")
plt.xlabel("Time Step")
plt.ylabel("Drawdown")
plt.legend()
plt.grid(True)
plt.show()

import pandas as pd
import plotly.graph_objects as go
from plotly.subplots import make_subplots

T = len(rl_equity)

df = pd.DataFrame({
    "t": np.arange(T),
    "price": price_series,

    "rl_equity": rl_equity,
    "bh_equity": bh_equity,
    "rand_equity": rand_equity,

    "rl_dd": rl_dd,
    "bh_dd": bh_dd,
    "rand_dd": rand_dd,

    "action": rl_actions,
    "step_pnl": rl_pnl
})

fig_equity = go.Figure()

fig_equity.add_trace(go.Scatter(
    x = df["t"], y = df["rl_equity"],
    name = "RL Agent",
    line = dict(width = 2)
))

fig_equity.add_trace(go.Scatter(
    x = df["t"], y = df["bh_equity"],
    name = "Buy & Hold"
))

fig_equity.add_trace(go.Scatter(
    x = df["t"], y = df["rand_equity"],
    name = "Random"
))

fig_equity.update_layout(
    title = "Portfolio Value vs Benchmarks",
    xaxis_title = "Time",
    yaxis_title = "Portfolio Value",
    hovermode = "x unified"
)

buy_mask = df["action"] == 1
sell_mask = df["action"] == 2

fig_actions = go.Figure()

fig_actions.add_trace(go.Scatter(
    x = df["t"], y = df["price"],
    name = "Price",
    line = dict(color = "black")
))

fig_actions.add_trace(go.Scatter(
    x = df.loc[buy_mask, "t"],
    y = df.loc[buy_mask, "price"],
    mode = "markers",
    name = "Buy",
    marker = dict(symbol = "triangle-up", size = 10)
))

fig_actions.add_trace(go.Scatter(
    x = df.loc[sell_mask, "t"],
    y = df.loc[sell_mask, "price"],
    mode = "markers",
    name = "Sell",
    marker = dict(symbol = "triangle-down", size = 10)
))

fig_actions.update_layout(
    title = "Actions Overlaid on Price",
    xaxis_title = "Time",
    yaxis_title = "Price",
    hovermode = "x unified"
)

fig_pnl = go.Figure()

fig_pnl.add_trace(go.Histogram(
    x = df["step_pnl"],
    nbinsx = 50,
    name = "PnL Distribution"
))

fig_pnl.update_layout(
    title = "PnL Distribuiton",
    xaxis_title = "PnL per Step",
    yaxis_title = "Frequency",
    bargap = 0.05
)

dashboard = make_subplots(
    rows = 3, cols = 1,
    shared_xaxes = True,
    vertical_spacing = 0.05,
    subplot_titles = [
        "Portfolio Value vs Benchmarks",
        "Actions Overlaid on Price",
        "PnL Distribution"
    ]
)

dashboard.add_annotation(
    x = 0.01, y = 1.12,
    xref = "paper", yref = "paper",
    text = f"<b>Sharpe Ratio:</b> {rl_sharpe:.3f}",
    showarrow = False,
    font = dict(size = 14)
)

dashboard.add_annotation(
    x = 0.30, y = 1.12,
    xref = "paper", yref = "paper",
    text = f"<b>Max Drawdown:</b> {rl_max_dd*100:.2f}%",
    showarrow = False,
    font = dict(size = 14)
)

for trace in fig_equity.data:
  dashboard.add_trace(trace, row = 1, col = 1)

for trace in fig_actions.data:
  dashboard.add_trace(trace, row = 2, col = 1)

for trace in fig_pnl.data:
  dashboard.add_trace(trace, row = 3, col = 1)

dashboard.update_layout(
    height = 900,
    hovermode = "x unified",
    showlegend = True
)

dashboard.show()

fig = make_subplots(
    rows = 3, cols = 1,
    shared_xaxes = False,
    vertical_spacing = 0.08,
    subplot_titles = [
        "Portfolio Value vs Benchmarks",
        "Actions Overlaid on Market Price",
        "PnL Distribution"
    ]
)

time = np.arange(T)

fig.add_trace(
    go.Scatter(x = time, y = rl_equity, name = "RL Agent", line = dict(width = 2)),
    row = 1, col = 1
)

fig.add_trace(
    go.Scatter(x = time, y = bh_equity, name = "Buy & Hold"),
    row = 2, col = 1
)

fig.add_trace(
    go.Scatter(x = time, y = rand_equity, name = "Random Agent"),
    row = 3, col = 1
)

fig.show()

buy_idx = np.where(rl_actions == 1)[0]

sell_idx = np.where(rl_actions == 2)[0]

fig.add_trace(
    go.Scatter(x = time, y = price_series, name = "Price", line = dict(color = "black")),
    row = 2, col = 1
)

fig.add_trace(
    go.Scatter(
        x = time[buy_idx],
        y = price_series[buy_idx],
        mode = "markers",
        marker = dict(symbol = "triangle-up", size = 10, color = "green"),
        name = "Buy"
    ),
    row = 2, col = 1
)

fig.add_trace(
    go.Scatter(
        x = time[sell_idx],
        y = price_series[sell_idx],
        mode = "markers",
        marker = dict(symbol = "triangle-down", size = 10, color = "red"),
        name = "Sell"
    ),
    row = 2, col = 1
)

fig.add_trace(
    go.Histogram(
        x = rl_pnl,
        nbinsx = 40,
        name = "PnL Distribution",
        marker_color = "steelblue"
    ),
    row = 3, col = 1
)

fig.update_layout(
    height = 900,
    hovermode = "x unified",
    title_text = "RL Trading Agent - Performance, Risk & Behavior Dashboard",
    showlegend = True
)

fig.update_xaxes(title_text = "Time", row = 3, col = 1)
fig.update_yaxes(title_text = "Portfolio Value", row = 1, col = 1)
fig.update_yaxes(title_text = "Price", row = 2, col = 1)
fig.update_yaxes(title_text = "PnL", row = 3, col = 1)

fig.show()
