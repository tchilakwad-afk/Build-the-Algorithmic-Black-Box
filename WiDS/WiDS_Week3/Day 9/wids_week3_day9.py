# -*- coding: utf-8 -*-
"""WiDS_Week3_Day9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kudo7OtKDjuz6mScKqWa23u9pRgUpXnn
"""

!pip install optuna
!pip install stable-baselines3 gymnasium

import optuna
import numpy as np

import gymnasium as gym
from gymnasium import spaces

from collections import defaultdict
from collections import deque

from stable_baselines3 import PPO
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.monitor import Monitor

class OrderBookMarket:
    def __init__(self, initial_price=100.0, tick_size=0.1):
        self.last_price = initial_price
        self.tick_size = tick_size
        self.bids = defaultdict(float)
        self.asks = defaultdict(float)

    def flush_book(self):
        self.bids.clear()
        self.asks.clear()

    def get_mid_price(self):
        best_bid = max(self.bids.keys()) if self.bids else self.last_price - 0.5
        best_ask = min(self.asks.keys()) if self.asks else self.last_price + 0.5
        return (best_bid + best_ask) / 2.0

    def post_limit_order(self, side, price, quantity):
        price = round(price / self.tick_size) * self.tick_size
        if side == "buy": self.bids[price] += quantity
        elif side == "sell": self.asks[price] += quantity

    def match_market_order(self, side, quantity):
        remaining = quantity
        exec_price = self.last_price

        if side == "buy":
            sorted_asks = sorted(self.asks.keys())
            for p in sorted_asks:
                if remaining <= 0: break
                matched = min(remaining, self.asks[p])
                self.asks[p] -= matched
                if self.asks[p] < 1e-9: del self.asks[p]
                remaining -= matched
                exec_price = p

        elif side == "sell":
            sorted_bids = sorted(self.bids.keys(), reverse=True)
            for p in sorted_bids:
                if remaining <= 0: break
                matched = min(remaining, self.bids[p])
                self.bids[p] -= matched
                if self.bids[p] < 1e-9: del self.bids[p]
                remaining -= matched
                exec_price = p

        self.last_price = exec_price
        return exec_price

class LOBTradingEnv(gym.Env):
  def __init__(self):
    super(LOBTradingEnv, self).__init__()

    self.action_space = spaces.Discrete(3)

    self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(5,), dtype=np.float32)

    self.market = OrderBookMarket()
    self.initial_cash = 10_000

  def reset(self, seed=None, options=None):
    super().reset(seed=seed)
    self.market = OrderBookMarket()
    self.inventory = 0
    self.cash = self.initial_cash
    self.step_count = 0

    self.momentum = 0.0

    self._generate_background_liquidity()

    return self._get_obs(), {}

  def _generate_background_liquidity(self):
    self.market.flush_book()
    mid = self.market.last_price

    self.momentum = (0.8 * self.momentum) + np.random.normal(0, 0.5)
    mid += self.momentum

    self.market.last_price = mid

    for i in range(1, 6):
      self.market.post_limit_order("sell", mid + i*0.2, 10)
    for i in range(1, 6):
      self.market.post_limit_order("buy", mid - i*0.2, 10)

  def _get_obs(self):
    mid_price = self.market.get_mid_price()
    portfolio_val = self.cash + (self.inventory * mid_price)

    return np.array([
        self.inventory / 10.0,
        self.cash / 10_000.0,
        mid_price / 100.0,
        portfolio_val / 10_000.0,
        self.momentum
    ], dtype=np.float32)

  def step(self, action):
    mid_before = self.market.get_mid_price()
    prev_val = self.cash + (self.inventory * mid_before)

    self._generate_background_liquidity()
    self.step_count += 1

    if action == 1:
      if self.cash > self.market.last_price:
        exec_price = self.market.match_market_order("buy", 1)
        self.inventory += 1
        self.cash -= exec_price
    elif action == 2:
      if self.inventory > -5:
        exec_price = self.market.match_market_order("sell", 1)
        self.inventory -= 1
        self.cash += exec_price

    mid_after = self.market.get_mid_price()
    current_val = self.cash + (self.inventory * mid_after)

    reward = current_val - prev_val

    reward -= 0.001 * abs(self.inventory)

    terminated = self.step_count >= 200
    truncated = False
    return self._get_obs(), reward, terminated, truncated, {}

def evaluate_agent(model, env, n_eval_episodes = 5):
  total_returns = []

  for i in range(n_eval_episodes):
    obs, _ = env.reset(seed = 42 + i)
    terminated = False
    truncated = False
    initial_val = 10_000

    while not (terminated or truncated):
      action, _ = model.predict(obs, deterministic = True)
      obs, reward, terminated, truncated, _ = env.step(action)

    final_val = obs[3] * 10_000.0
    total_returns.append(final_val - initial_val)

  return np.mean(total_returns)

def objective(trial):
  learning_rate = trial.suggest_float("learning_rate", 1e-5, 1e-3, log = True)
  gamma = trial.suggest_float("gamma", 0.90, 0.999)
  ent_coef = trial.suggest_float("ent_coeff", 1e-4, 1e-2, log = True)

  train_env = Monitor(LOBTradingEnv())
  eval_env = Monitor(LOBTradingEnv())

  model = PPO(
      "MlpPolicy",
      train_env,
      learning_rate = learning_rate,
      gamma = gamma,
      ent_coef = ent_coef,
      n_steps = 2048,
      batch_size = 64,
      verbose = 0,
      seed = None
  )

  model.learn(total_timesteps = 50_000)

  score = evaluate_agent(model, eval_env, n_eval_episodes = 5)

  return score

study = optuna.create_study(
    direction = "maximize",
    sampler = optuna.samplers.TPESampler(seed = 42)
)

study.optimize(objective, n_trials = 25)

print("Best trial: ")
print(study.best_trial)

import pandas as pd

df = study.trials_dataframe()
df = df.sort_values("value", ascending = False)

df[[
    "value",
    "params_learning_rate",
    "params_gamma",
    "params_ent_coeff"
]].head(10)

import optuna.visualization as vis

vis.plot_optimization_history(study)

vis.plot_param_importances(study)