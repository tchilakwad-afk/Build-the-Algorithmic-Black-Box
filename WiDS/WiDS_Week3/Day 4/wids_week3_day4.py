# -*- coding: utf-8 -*-
"""WiDS_Week3_Day4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f_kALpeI5ogJKT9bh7EeLb45NnHPLAs2
"""

!pip install stable-baselines3 gymnasium

import gymnasium as gym
from gymnasium import spaces
import numpy as np

from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.monitor import Monitor

class TradingEnv(gym.Env):
  metadata = {"render_modes": []}

  def __init__(self, max_steps = 100):
    super().__init__()

    self.initial_cash = 10_000.00
    self.initial_price = 100.00
    self.max_steps = max_steps
    self.prev_portfolio_value = None
    self.k_inventory = 0.01
    self.k_risk = 0.1
    self.peak_portfolio_value = None

    self.action_space = spaces.Discrete(3)

    self.observation_space = spaces.Box(
        low = np.array([0.0, -1.0, 0.0], dtype = np.float32),
        high = np.array([2.0, 1.0, 2.0], dtype = np.float32)
    )

    self.reset()

  def reset(self, seed = None, options = None):
    super().reset(seed = seed)

    self.price = self.initial_price
    self.inventory = 0
    self.cash = self.initial_cash
    self.timestamp = 0
    self.prev_portfolio_value = self._portfolio_value()
    self.peak_portfolio_value = self._portfolio_value()

    obs = self._get_obs()
    info = {}

    return obs, info

  def step(self, action):
    assert self.action_space.contains(action)

    prev_value = self._portfolio_value()

    if action == 1:
      if self.cash >= self.price:
        self.cash -= self.price
        self.inventory += 1

    elif action == 2:
      if self.inventory > 0:
        self.cash += self.price
        self.inventory -= 1

    self.price += np.random.normal(0, 0.5)
    self.price = max(1.0, self.price)

    current_value = self._portfolio_value()
    delta_pnl = current_value - self.prev_portfolio_value
    self.prev_portfolio_value = current_value

    self.timestamp += 1

    inventory_penalty = self.k_inventory * abs(self.inventory)
    risk_penalty = self.k_risk * (delta_pnl ** 2)

    reward = delta_pnl - inventory_penalty - risk_penalty

    self.peak_portfolio_value = max(self.peak_portfolio_value, current_value)
    drawdown = self.peak_portfolio_value - current_value

    terminated = False
    truncated = self.timestamp >= self.max_steps

    obs = self._get_obs()

    info = {
        "price": self.price,
        "inventory": self.inventory,
        "cash": self.cash,
        "portfolio_value": current_value,
        "raw_pnl": delta_pnl,
        "inventory_penalty": inventory_penalty,
        "risk_penalty": risk_penalty,
        "drawdown": drawdown
    }

    return obs, reward, terminated, truncated, info

  def _portfolio_value(self):
    return self.cash + self.inventory * self.price

  def _get_obs(self):
    price_norm = self.price / self.initial_price
    inventory_norm = self.inventory / 10.0
    cash_norm = self.cash / self.initial_cash

    return np.array(
        [price_norm, inventory_norm, cash_norm],
        dtype = np.float32
    )

def make_env():
  env = TradingEnv(max_steps = 100)
  env = Monitor(env)
  return env

vec_env = make_vec_env(make_env, n_envs = 1)

model = PPO(
    policy = "MlpPolicy",
    env = vec_env,
    verbose = 1,
    n_steps = 128,
    batch_size = 64,
    learning_rate = 3e-4,
    gamma = 0.99
)

model.learn(total_timesteps = 5_000)

obs = vec_env.reset()

for _ in range(20):
  action, _ = model.predict(obs, deterministic = True)
  obs, reward, done, info = vec_env.step(action)