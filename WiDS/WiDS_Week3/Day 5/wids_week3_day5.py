# -*- coding: utf-8 -*-
"""WiDS_Week3_Day5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EXLS5Z5mez5tq99r7dyXaaR1H3BV5ITD
"""

!pip install stable-baselines3 gymnasium

import gymnasium as gym
from gymnasium import spaces
import numpy as np

from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.callbacks import BaseCallback

class TradingEnv(gym.Env):
  metadata = {"render_modes": []}

  def __init__(self, max_steps = 100):
    super().__init__()

    self.initial_cash = 10_000.00
    self.initial_price = 100.00
    self.max_steps = max_steps
    self.prev_portfolio_value = None
    self.k_inventory = 0.001
    self.k_risk = 0.01
    self.peak_portfolio_value = None

    self.action_space = spaces.Discrete(3)

    self.observation_space = spaces.Box(
        low = -np.inf,
        high = np.inf,
        shape = (3, ),
        dtype = np.float32
    )

    self.reset()

  def reset(self, seed = None, options = None):
    super().reset(seed = seed)

    self.price = self.initial_price
    self.inventory = 0
    self.cash = self.initial_cash
    self.timestamp = 0

    self.portfolio_value = self._calc_portfolio_value()

    self.prev_portfolio_value = self.portfolio_value
    self.peak_portfolio_value = self.portfolio_value

    obs = self._get_obs()
    info = {}

    return obs, info

  def step(self, action):
    self.timestamp += 1
    self.price = 100 + 20 * np.sin(self.timestamp * 0.2)

    if action == 1:
      if self.cash >= self.price:
        self.cash -= self.price
        self.inventory += 1

    elif action == 2:
      if self.inventory > 0:
        self.cash += self.price
        self.inventory -= 1

    current_value = self._calc_portfolio_value()

    delta_pnl = current_value - self.prev_portfolio_value
    self.prev_portfolio_value = current_value

    inventory_penalty = self.k_inventory * abs(self.inventory)
    risk_penalty = self.k_risk * (delta_pnl ** 2)

    reward = delta_pnl - inventory_penalty - risk_penalty

    self.peak_portfolio_value = max(self.peak_portfolio_value, current_value)
    drawdown = self.peak_portfolio_value - current_value

    terminated = False
    truncated = self.timestamp >= self.max_steps

    obs = self._get_obs()

    info = {
        "price": self.price,
        "inventory": self.inventory,
        "cash": self.cash,
        "portfolio_value": current_value,
        "raw_pnl": delta_pnl,
        "inventory_penalty": inventory_penalty,
        "risk_penalty": risk_penalty,
        "drawdown": drawdown
    }

    return obs, reward, terminated, truncated, info

  def _calc_portfolio_value(self):
    return self.cash + self.inventory * self.price

  def _get_obs(self):
    price_norm = self.price / self.initial_price
    inventory_norm = self.inventory / 10.0
    cash_norm = self.cash / self.initial_cash

    return np.array(
        [price_norm, inventory_norm, cash_norm],
        dtype = np.float32
    )

class EntropyCallBack(BaseCallback):
  def __init__(self):
    super().__init__()
    self.entropies = []
    self.timesteps = []

  def _on_step(self) -> bool:
    if "train/entropy_loss" in self.logger.name_to_value:
      entropy = self.logger.name_to_value["train/entropy_loss"]
      self.entropies.append(entropy)
      self.timesteps.append(self.num_timesteps)
    return True

import os
import time

log_dir = f"./logs/run_{int(time.time())}"
os.makedirs(log_dir, exist_ok = True)

env = make_vec_env(lambda: Monitor(TradingEnv(max_steps = 100), log_dir), n_envs = 1)

model = PPO(
    "MlpPolicy",
    env,
    verbose = 1,
    learning_rate = 3e-4,
    gamma = 0.99,
    ent_coef = 0.01
)

model.learn(total_timesteps = 50_000)

entropy_cb = EntropyCallBack()

model.learn(
    total_timesteps = 50_000,
    callback = entropy_cb
)

import pandas as pd
import matplotlib.pyplot as plt

log_df = pd.read_csv("monitor.csv", skiprows = 1)
log_df.head()

log_df["r_smooth"] = log_df["r"].rolling(10).mean()
plt.plot(log_df["t"], log_df["r_smooth"])
plt.xlabel("Timesteps")
plt.ylabel("Episode Reward")
plt.title("Episode Reward vs Time")
plt.show()

plt.plot(entropy_cb.timesteps, entropy_cb.entropies)
plt.xlabel("Timesteps")
plt.ylabel("Entropy Loss")
plt.title("Policy Entropy vs Time")
plt.show()

obs = env.reset()
action_counts = {0: 0, 1: 0, 2: 0}

for _ in range(500):
  action, _ = model.predict(obs, deterministic = True)
  action_counts[int(action)] += 1
  obs, _, dones, _ = env.step(action)
  if dones[0]:
    break

action_counts